{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dynamic Programming and Structural Econometrics #4\n",
    "\n",
    "### Multidimensional Numerical Integration: Monte Carlo and Quadrature\n",
    "\n",
    "**Readings:** \n",
    "- ðŸ“– Judd, K. L. (1998). Numerical methods in economics. MIT press. Sections 7.1-7.5, 8.2\n",
    "\n",
    "by Bertel Schjerning\n",
    "\n",
    "University of Copenhagen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### This Lecture: Multidimensional integration\n",
    "$$\n",
    "\\int \\cdots \\int f(x_1,\\cdots, x_d) g(x_1,\\cdots, x_d) dx_1 \\cdots dx_d\n",
    "$$\n",
    "\n",
    "1. Multidimensional Quadrature (product rule) $\\rightarrow$ curse of dimensionality\n",
    "1. Monte Carlo $\\rightarrow$ breaks curse of dimensionality\n",
    "1. Sampling from a multivariate distributions (leading example: normal and log-normal)\n",
    "1. Dependency for random grids in higher dimensions (compare random, sobol and halton)\n",
    "1. Illustrate that MC breaks curse of dimensionality (root n convergence) \n",
    "1. Compare MC with sparse grid for test problems\n",
    "1. Compare methods on test problems with varying dimensionality\n",
    "1. Applications: Port-folio choice model (Static) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multidimensional quadrature\n",
    "\n",
    "Much more complication, simple methods are subject to **curse of dimensionality**\n",
    "\n",
    "- Generic product rule for quadrature\n",
    "\n",
    "$$\n",
    "\\int_{[a,b]^d}f(x)dx \\approx \\sum_{i_1=1}^n \\dots\\sum_{i_d=1}^n \\omega_{i_1}^1\\dots\\omega_{i_d}^d f(x_{i_1}^1,\\dots,x^d_{i_d})\n",
    "$$\n",
    "\n",
    "- Product rule for Gaussian quadrature is based on a full tensor product of orthogonal polynomials  \n",
    "- Sparse girds: Smolyaks algorithm \n",
    "- Monte Carlo integration!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Product rule subject to curse of dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "n=2 # number of gridpoints in each dimension\n",
    "print('%-10s%15s' % ('d', '# of nodes'))\n",
    "for d in range(1, 10): print(\"%-10d%15g\" % (d, n**d))\n",
    "d=10; print(\"%-10d%15g\" % (d, n**d))\n",
    "d=20; print(\"%-10d%15g\" % (d, n**d))\n",
    "d=30; print(\"%-10d%15g\" % (d, n**d))\n",
    "d=50; print(\"%-10d%15g\" % (d, n**d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Monte Carlo\n",
    "Addresses the computation of a multidimensional definite integral\n",
    "\n",
    "$$\n",
    "I_f\n",
    "=\\int _{\\Omega}f(\\mathbf{x})g(\\mathbf{x})\\,d\\mathbf{x}\n",
    "=E[f(\\mathbf{x})]\n",
    "$$\n",
    "\n",
    "where $g(\\mathbf{x})$ is the density for $d$-dimensional random variable $\\mathbf{x}$ with support $\\Omega$. \n",
    "\n",
    "In the special case where $\\mathbf{x}$ uniformly distributed on $\\Omega=[0,1]^d$ we have\n",
    "\n",
    "$$I_f\n",
    "=\\int _{[0,1]^d}f(\\mathbf{x})\\,d\\mathbf{x}\n",
    "$$\n",
    "\n",
    "### Monte Carlo integration is simple\n",
    "1. sample $N$ points $\\mathbf{x}_1,\\cdots,\\mathbf{x}_N$ from $g(\\mathbf{x})$ \n",
    "1. approximate the mean $E[f(\\mathbf{x}])$ by the sample average\n",
    "\n",
    "$$\n",
    "I\\approx Q_{N}\\equiv {\\frac  {1}{N}}\\sum _{{i=1}}^{N}f(\\mathbf  {x}_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Monte Carlo breaks the Curse of dimensionality\n",
    "\n",
    "$\\textbf{Consistency}$: Law of large numbers ensures that the sample average converge to the mean\n",
    "\n",
    "$$\\lim _{{N\\to \\infty }}Q_{N}\n",
    "=\\lim _{{N\\to \\infty }}{\\frac  {1}{N}}\\sum _{{i=1}}^{N}f(\\mathbf{x}_{i})\n",
    "=E[f(\\mathbf{x})]\n",
    "=I_f$$\n",
    "\n",
    "$\\textbf{Asymptotic Normality}$ : By the central limit theorem we have\n",
    "$$\n",
    "\\sqrt{N}\\left(Q_N-I_f \\right)\\ \\xrightarrow {d} \\ N\\left(0,\\sigma ^{2}\\right)\n",
    "$$\n",
    "where $\\sigma= \\operatorname {Var}[f(\\mathbf{x})]$ does not depend on $N$ and $d$\n",
    "\n",
    "So that the standard error of $Q_N$ \n",
    "$$\\sigma_{Q_N}=\\frac{{\\sigma}}{\\sqrt{N}}$$ \n",
    "decreases with the standard parametric rate, $1/\\sqrt{N}$\n",
    "\n",
    "- The result that $\\sigma_{Q_N}$ decreases with $1/\\sqrt{N}$ and $\\sigma ^{2}$ does NOT depend on the number of dimensions of the integral! \n",
    "\n",
    "- This is a huge advantage of Monte Carlo integration compared to most deterministic methods that depend exponentially on the dimension.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some random generators in Python\n",
    "\n",
    "Modules:\n",
    "- Random - basic functionality, scalar numbers\n",
    "- NumPy.random - vectorized, many distributions\n",
    "- SciPy.stats - more probability and statistics functions\n",
    "- chaospy - advanced Monte Carlo integration \n",
    "(random, Hammersley, Halton, Sobol, Regular, Chebyshev, ...)\n",
    "\n",
    "Installation of chaospy should be straight forward from terminal:\n",
    "\n",
    "`pip install chaospy`\n",
    "\n",
    "Some people may experience dependency problmes. Code below worked for me (use at own risk) <br>\n",
    "`pip uninstall -y numpy numpoly chaospy` <br>\n",
    "`pip install numpy==1.26.4` <br>\n",
    "`pip install numpoly==1.2.12` <br>\n",
    "`pip install chaospy` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Most of what we do today relies on these libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "# !pip install chaospy # uncomment to install\n",
    "import chaospy  ## needs installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Need this function to plot some distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def hist(data,bins='auto',range=None,theoretical=None,cdf=False):\n",
    "    '''Draws histogram of data, imposes a theoretical distribution if given'''\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    if cdf:  # plot CDF instead of histogram\n",
    "        plt.hist(data,bins=len(data),range=range,cumulative=True,density=True,align='right',histtype='step',color='black')\n",
    "    else:\n",
    "        plt.hist(data,bins=bins,range=range,density=True,histtype='bar',color='lightgrey',edgecolor='k')\n",
    "    if theoretical and len(data)>0:  # add theoretical distribution\n",
    "        x = (np.linspace(range[0],range[-1],100) if range else np.linspace(min(data),max(data),100))\n",
    "        y = theoretical(x)\n",
    "        plt.plot(x,y,'r-')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's start simple: Sample from the uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "N=1000\n",
    "x = np.random.random(N) #NumPy\n",
    "hist(x,bins=10, theoretical=scipy.stats.uniform.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other distributions\n",
    "\n",
    "- Normal  \n",
    "- Log-normal  \n",
    "- Exponential  \n",
    "- Fisher  \n",
    "- $ \\chi^2 $-distribution  \n",
    "\n",
    "\n",
    "and many other\n",
    "\n",
    "[https://docs.scipy.org/doc/numpy/reference/routines.random.html] <br/>\n",
    "[https://chaospy.readthedocs.io/en/master/distributions/index.html]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simulate from log-normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# simulate from log-normal \n",
    "for n in [10, 10**6]:\n",
    "    x_sim = np.random.lognormal(size=n)\n",
    "    hist(x_sim,bins=25,range=(0,5),theoretical=lambda x: scipy.stats.lognorm.pdf(x,loc=0, s=1.0))\n",
    "    plt.title('%d realizations'%n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inverse transform sampling\n",
    "- Simulating from distribution based on uniform random numbers\n",
    "\n",
    "- Let $ F(x) $ be cdf of the random variable of interest\n",
    "$ X $, with inverse $ x=F^{(-1)}(u) $\n",
    "\n",
    "- To simulate $ X $:\n",
    "    1. simulate $ (u_1,\\dots,u_n) $ from standard uniform distribution  \n",
    "    1. return $ \\big( F^{(-1)}(u_1),\\dots,F^{(-1)}(u_n) \\big) $  \n",
    "\n",
    "$$\n",
    "X = F^{(-1)}(U)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\{X \\le x \\}) = P(\\{ F^{(-1)}(U) \\le x \\}) =\n",
    "$$\n",
    "\n",
    "$$\n",
    "= P(\\{ U \\le F(x) \\}) = F_U \\big(F(x)\\big) = F(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inverse CDF\n",
    "\n",
    "<center><img src=\"img/invcdf.png\" style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples\n",
    "### Normal distribution\n",
    "$$\n",
    "u=F(x) = \\Phi \\big(\\frac{x-\\mu}{\\sigma}  \\big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "x=F^{-1}(u) =(\\mu + \\sigma \\Phi^{-1}(u))\n",
    "$$\n",
    "\n",
    "### Log-normal distribution\n",
    "$$\n",
    "u=F(x) = \\Phi \\big(\\frac{\\ln(x)-\\mu}{\\sigma}  \\big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "x=F^{-1}(u) =\\exp(\\mu + \\sigma \\Phi^{-1}(u))\n",
    "$$\n",
    "\n",
    "###  Gumbel distribution\n",
    "$$\n",
    "u=F(x) = \\exp \\big( -\\exp (-\\frac{x-\\mu}{\\beta} ) \\big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "x=F^{-1}(u) = \\mu - \\beta \\ln( -\\ln(u))\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simulate from log-normal using inverse transform sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def lognormal(n=1000, Î¼=0, Ïƒ=0.5): # simulate x as log normal such that ln(x) ~ N(mu, sigma)\n",
    "    u = np.random.random(size=n)   # simulate uniform draws  \n",
    "     # transform uniform draws to lig normal draws based in inverse cdf of log-normal     \n",
    "    return np.exp(Î¼+ Ïƒ*scipy.stats.norm.ppf(u));\n",
    "n=10000; Ïƒ=1; Î¼=0 # note that scipy parameterizes log-normal differerentby shifting with so not equivalent unless Î¼=0  \n",
    "x = lognormal(n, Î¼, Ïƒ)\n",
    "hist(x,bins=25,range=(0,5),theoretical=lambda x: scipy.stats.lognorm.pdf(x, s=Ïƒ))\n",
    "plt.title('Lognormal, %d realizations'%N);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Repeatability of simulations\n",
    "\n",
    "Sometimes it is important to be able to repeat the simulationâ€™s random\n",
    "number sequence\n",
    "- **get_state()**\n",
    "- **set_state()**\n",
    "- **seed()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Draw log-normals with fixed seed\n",
    "Ïƒ=1; \n",
    "N=50;\n",
    "st = np.random.get_state()\n",
    "for i in range(2):    \n",
    "    np.random.set_state(st) # uncomment to set state to st to use fixed for both graphs (varies from call to call)\n",
    "    np.random.seed(2008)  # uncomment to use fixed seed in both graphs for all calls\n",
    "    x = lognormal(n=N, Î¼=0, Ïƒ=Ïƒ)\n",
    "    hist(x,bins=25,range=(0,5),theoretical=lambda x: scipy.stats.lognorm.pdf(x,s=Ïƒ))\n",
    "    plt.title('Lognormal, %d realizations'%N);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random draws, systematic draws and variance reduction techniques\n",
    "- **Independent random draws**: possible that the draws will be clumped together, with no draws from large areas of the domain.  \n",
    "- **Halton draws** - good coverage in low dimensions... but tend to create artificial correlations in higher dimnensions\n",
    "- **Sobol draws** - Sobol sequence seems better than Halton sequences except from the first dimensions.\n",
    "- **antithetics** - improve converage by creating negative correlation between draws by mirroring draws\n",
    "\n",
    "\n",
    "\n",
    "- Monte Carlo using **random draws** converge at a rate, ${\\frac  {1}{{\\sqrt  {N}}}}$\n",
    "\n",
    "- Monte Carlo using **pseudo random draws** converge at a rate, ${\\frac{(\\log N)^{d}}{N}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Function for plot quadrature nodes and uniform (pseudo) random draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_draws(n=20, d=2, transform=None, print_out=False, \n",
    "               rules=['legendre_tensor', 'legendre_sparse', \"random\", \"halton\", \"sobol\"], dims=(1,2)):\n",
    "    fig, ax = plt.subplots(1,len(rules),figsize=(20,4))\n",
    "    for idx, rule in enumerate(rules):\n",
    "        \n",
    "        d=max(max(dims),d)\n",
    "        \n",
    "        distribution = chaospy.Iid(chaospy.Uniform(0, 1), d)\n",
    "        if rule=='legendre_tensor':  # legendre nodes on [a,b] interval\n",
    "            order=int(n**(1/d))-1\n",
    "            x, w = chaospy.generate_quadrature(order, distribution, rule=(\"legendre\"), sparse=False)\n",
    "        elif rule=='legendre_sparse':  # legendre nodes on [a,b] interval\n",
    "            order=int(n**(1/d))-1\n",
    "            x, w = chaospy.generate_quadrature(order, distribution, rule=(\"legendre\"), sparse=True)\n",
    "        else:  # take random draws (dxN)\n",
    "            order=n\n",
    "            x=chaospy.generate_samples(n, domain=d, rule=rule)  # take random draws (dxN)\n",
    "            w=np.array(1/x.shape[1])\n",
    "\n",
    "        if transform:\n",
    "            x = transform(x);\n",
    "\n",
    "        # figure title\n",
    "        fig.suptitle('Dimensions ' + str(dims[0]) + ' and ' + str(dims[1]))\n",
    "\n",
    "        # Show samples as 2-dimensional scatter plot\n",
    "        ax[idx].scatter(x[dims[0]-1,:], x[dims[1]-1,:], marker=\".\", color=\"b\", s=5)\n",
    "\n",
    "        # Add labels and title\n",
    "        ax[idx].set_xlabel(\"dimension \" + str(dims[0]))\n",
    "        ax[idx].set_ylabel(\"dimension \" + str(dims[1])) if not idx else None\n",
    "        ax[idx].set_aspect(\"equal\")\n",
    "        ax[idx].title.set_text(rule)\n",
    "        if print_out: \n",
    "            print('dim(x)=%3d' %d, 'order= %-5d' % order, 'N== %-5d' % x.shape[1], ', rule=', rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_draws(n=5**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Drawing from mutivariate normals\n",
    "Consider a multivariate normal distrubution $N(\\mu,\\Sigma)$ where $\\mu=(\\mu_1,\\cdots, \\mu_d)'$ is a $d \\times 1$ vector of means and $\\Sigma$ is the $d \\times d$ covariance matrix \n",
    "\n",
    "A $d \\times N$ vector of draws $\\epsilon \\sim N(\\mu,\\Sigma)$ is obtained as follows:\n",
    "1. Compute the lower triangular Cholesky matrix $L$ defined such that $LL' = \\Sigma$\n",
    "1. Generate $d \\times N$ matrix $u$ with iid. draws from the uniform distribution, $U(0,1)$  \n",
    "1. Transform $u$ by computing $\\eta=\\Phi^{-1}(u)$ using the inverse cdf from the standard normal \n",
    "1. Calculate $\\epsilon = \\mu + L\\eta$ \n",
    "\n",
    "We can verify the properties of $\\epsilon$:\n",
    "- It is normally distributed, since the sum of normals is normal.\n",
    "- It mean is $\\mu$: $E(\\epsilon) = \\mu + LE(\\eta) = \\mu$. \n",
    "- It variance is $\\Sigma$: $Var(\\epsilon) =L Var(\\eta)L' =LL' = \\Sigma$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Build $\\Sigma_{d \\times d}$ with constant variance $\\sigma^2/d $ and pairwise correlation $c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def build_sigma(d=2, Ïƒ = 1, c=0, print_out=False): \n",
    "    '''build d-dimensional (dxd) covariance matrix with correlation c between variables'''\n",
    "    if (c<0.0) and not (d==2):\n",
    "        raise RuntimeError('build_sigma can only create negaive correlation for d=2')\n",
    "        \n",
    "    Ïƒ = Ïƒ/np.sqrt(d) * np.ones((d,1))             # vector of standard dev. (divide by sqrt(d) to keep std of sum constant)\n",
    "    corr=(1-c)*np.identity(d) + c*np.ones((d,d)); # correlation martrix    \n",
    "    Î£=Ïƒ*corr*Ïƒ.T                                  # variance-covariance martrix with correlation as in corr\n",
    "    L=np.linalg.cholesky(Î£)                       # Lower triangular cholesky matrix, with property: LÂ @ L.T=sigma2 (i.e. \"square root\" of matrix)\n",
    "    if print_out:\n",
    "        print('Correlation matrix\\n',   corr.round(4))\n",
    "        print('Covariance matrix\\n',  Î£.round(4))\n",
    "        print('Cholesky matrix, L\\n',   L.round(4))    \n",
    "    return Î£\n",
    "\n",
    "Î£ = build_sigma(d=4, Ïƒ = 1, c=0.5, print_out=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transforming uniform draws to mutivariate normal distribution, $x \\sim N(\\mu, \\Sigma)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def inv_multnorm(u, Î¼, Î£): \n",
    "    L=np.linalg.cholesky(Î£)  # Lower triangular cholesky matrix L@L.T=Î£ (i.e. \"square root\" of Î£)\n",
    "    return Î¼ + L @ scipy.stats.norm.ppf(u); # transform uniform to mutivariate normal \n",
    "\n",
    "d=3;      # dimension of x\n",
    "N=10000   # number of samples\n",
    "Î£ = build_sigma(d, Ïƒ = 1, c=.5, print_out=True) # Variance-covariance matrix\n",
    "u=np.random.random(size=(d,N)) # Generate uniform samples, u ~ U[0,1]\n",
    "x=inv_multnorm(u, Î¼=1, Î£=Î£)       # Transform u to multivarite Normal with x~N(Î¼, Î£)\n",
    "print('Covariance matrix (simulated data)\\n', np.cov(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Monte Carlo provide better coverage for high dimensional problems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "d=5; n=5**d;      # make sure n is not to large, try d = 2 and 5\n",
    "# d=20;  n=2**d;  # make sure n is not to large, try d= 10-20\n",
    "c=.7             # try changing c (negative correlation only works for d=2)\n",
    "Î£ = build_sigma(d, Ïƒ = 1, c=c, print_out=False) # build variance-covariance matrix\n",
    "plot_draws(n, d, dims=(1,2), print_out=True)\n",
    "plot_draws(n, d, dims=(1,2), transform = lambda u: inv_multnorm(u, Î¼=1, Î£=Î£))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some pseudo random grids have problems with correlation in higher dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=21 # try 2, 21, 100 and 200\n",
    "n=1000;\n",
    "Î£ = build_sigma(d, Ïƒ = 1, c=.9, print_out=False) # build variance-covariance matrix\n",
    "plot_draws(n, d, dims=(d-1,d), rules=[\"random\", \"halton\", \"sobol\"])\n",
    "plot_draws(n, d, dims=(d-1,d), transform=lambda u: inv_multnorm(u, Î¼=1, Î£=Î£), rules=[\"random\", \"halton\", \"sobol\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's do some Legendre quadrature using chaospy : Solve $\\int_a^b 1/d \\sum_{i=1}^d x_i^m dx_1,\\dots,dx_d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def gauss_legendre(f, order=5, d=1, a=0, b=1, sparse=False, output=False, true_val=None):\n",
    "    '''Compute the integral on [a, b]^d using gaussian quadrature with sample size N'''\n",
    "    distribution = chaospy.Iid(chaospy.Uniform(0, 1), d)\n",
    "    x0, w0 = chaospy.generate_quadrature(order, distribution, 'legendre', sparse)\n",
    "    x=(b-a)*x0+a      # adjust to interval [a, b]\n",
    "    fx=f(x)           # evaluate f(x) adjusted to interval [a, b]\n",
    "    Qn = np.sum((b-a)*fx*w0) # quadrture rule \n",
    "    \n",
    "    if output == True:         \n",
    "        if not sparse: print('Using Legendre nodes with (d,N,order)=(%d,%d,%d) based on tensor product'%(x.shape[0],x.shape[1], order))\n",
    "        if sparse: print('Using Legendre nodes with (d,N,order)=(%d,%d,%d) based on sparse Smolyack grid '%(x.shape[0],x.shape[1], order))\n",
    "        if true_val!=None:  \n",
    "            print('%-15s%20s%20s' % ('True value', 'Quadrature', 'Bias'))\n",
    "            print('%-15g%20g%20g\\n' % (true_val, Qn, (Qn-true_val)))\n",
    "        else:\n",
    "            print('%-15s%20s' % ('Estimate'))\n",
    "            print('%-15g%20g\\n' % (Qn.round(10)))\n",
    "    return Qn\n",
    "# We should be able to approximate an integral whose integrand can be perectly approximated \n",
    "# by a polynomial of degree m=2*(order+1)-1 \n",
    "d=5 # Try d = 5-10\n",
    "N=5**d                     # Total number of nodes in tensor grid\n",
    "order=int((N-d)**(1/d))    # order that gives N nodes in tensor product basis\n",
    "a=-1; b=2; m=9;            # try different number of draws N, number of dimensions d, and exponents m\n",
    "f= lambda x: np.mean(x**m, axis=0);  # function to integrate from a to b\n",
    "for sparse in [True, False]:\n",
    "    Qn= gauss_legendre(f, order, d, a, b, sparse, output=True, true_val=1/(m+1)*(b**(m+1)-a**(m+1))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's kick start our MC: solve 1000 dimensional integral $\\int_a^b 1/d \\sum_{i=1}^d x_i^m dx_1,\\dots,dx_d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def mc_integral(f, N=1000, d=1, rule='random', a=0, b=1, output=False, true_val=None):\n",
    "    '''Compute the integral on [a, b]^d using uniform Monte Carlo with sample size N'''\n",
    "    # Generate a d x N matrix of uniform random (or quasi-random) numbers according to rule \n",
    "    x0=chaospy.generate_samples(order=N, domain=d, rule=rule) # rule can for example be \"random\", \"halton\", \"sobol\"\n",
    "    x=(b-a)*x0+a      # adjust to interval [a, b]\n",
    "    fx=(b-a)*f(x)     # evaluate f(x) adjusted to interval [a, b]\n",
    "    Qn = np.sum(fx/N)            # estimate of integral\n",
    "    se_Qn=np.std(fx)/np.sqrt(N)  # standard errors (only valid for rule == 'random' !!!!)\n",
    "    if output == True:         \n",
    "        print('Using (N,d)=(%d,%d) %s draws'%(N,d,rule))\n",
    "        if true_val!=None:  \n",
    "            print('%-15s%20s%20s%20s' % ('True value', 'Estimate', 'Bias', 'Std. error'))\n",
    "            print('%-15g%20g%20g%20g\\n' % (true_val, Qn, (Qn-true_val), se_Qn))\n",
    "        else:\n",
    "            print('%-15s%20s' % ('Estimate', 'Std. error'))\n",
    "            print('%-15g%20g\\n' % (Qn.round(10) , se_Qn.round(10)))\n",
    "    return Qn, se_Qn\n",
    "\n",
    "d=1000; N = 1000; a=-1; b=2; m=5;    # try different number of draws N, number of dimensions d, and exponents m\n",
    "f= lambda x: np.mean(x**m, axis=0);  # function to integrate from a to b\n",
    "for rule in ['random', 'halton', 'sobol']:\n",
    "    if not (d>=1111 and rule=='sobol'): # sobol not available for d>=1111\n",
    "        Qn, se_QN= mc_integral(f, N, d, rule, a, b, output=True, true_val=1/(m+1)*(b**(m+1)-a**(m+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's compare quadrature and Monte Carlo (for d-dimensional integral)\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty} \\cdots \\int_{-\\infty}^{\\infty} f(x_1,\\cdots, x_d) g(x_1) \\cdots g(x_d) dx_1 \\cdots dx_d\n",
    "$$\n",
    "- **Test problem 1**: Compute integral of sum of d normal distributed variables  \n",
    "$$\n",
    "f(x)=f(x_1,\\cdots, x_d)=\\sum_{i=1}^d x_i\n",
    "$$\n",
    "and \n",
    "$g(x)$ is the density of the normal distribution with mean $\\mu$ and variance $\\sigma^2$\n",
    "\n",
    "- **Test problem 2:** Compute integral of product of $d$ log-normal distribution, i.e. where\n",
    "$$\n",
    "f(x)=f(x_1,\\cdots, x_d)=\\Pi_{i=1}^d \\exp(x_i)=\\exp\\left(\\sum_{i=1}^d x_i\\right)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test1 = False;   # Set to True to do Test model 1, else do Test model 2\n",
    "d=5; N=5**d;     # make sure N is not to large, try d = 2 and 5\n",
    "# d=10;  N=2**d;  # make sure N is not to large, try d= 10-20\n",
    "order=int((N-d)**(1/d))   # order that gives N nodes in tensor product basis\n",
    "Î¼=1.0/d   # normalize Î¼ with 1/d to hold mean of sum of x constant when d is increased\n",
    "Î£ = build_sigma(d, Ïƒ = 1, c=0.0) # build variance-covariance matrix\n",
    "\n",
    "if test1: \n",
    "    f = lambda u: np.sum(inv_multnorm(u, Î¼=Î¼, Î£=Î£) , axis=0);   # Test problem 1 (product of d log-normals)\n",
    "    true_val=np.array(Î¼)*d\n",
    "else:\n",
    "    f = lambda u: np.exp(np.sum(inv_multnorm(u, Î¼=Î¼, Î£=Î£) , axis=0)); # Test problem 2 (product of d log-normals)\n",
    "    one=np.ones((d,1)) # vector of ones used in variance formula var(sum_d(x))=a.T@Î£@a\n",
    "    true_val=np.array(np.exp(Î¼*d + one.T@Î£@one/2).sum())\n",
    "\n",
    "a=0;b=1;  # intergarte over support of uniform distribution               \n",
    "Qn = gauss_legendre(f, order, d, a=a, b=b, sparse=True, output=True, true_val=true_val) \n",
    "Qn = gauss_legendre(f, order, d, a=a, b=b, sparse=False, output=True, true_val=true_val) \n",
    "for rule in ['random', 'halton', 'sobol']:\n",
    "    Qn, se_QN= mc_integral(f, N, d, rule, a, b, output=True, true_val=true_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall Properties of Monte Carlo\n",
    "\n",
    "$\\textbf{Consistency}$: Law of large numbers ensures that the sample average converge to the mean\n",
    "\n",
    "$$\\lim _{{N\\to \\infty }}Q_{N}\n",
    "=\\lim _{{N\\to \\infty }}{\\frac  {1}{N}}\\sum _{{i=1}}^{N}f(\\mathbf{x}_{i})\n",
    "=E[f(\\mathbf{x})]\n",
    "=I_g$$\n",
    "\n",
    "$\\textbf{Asymptotic Normality}$ : By the central limit theorem we have\n",
    "$$\n",
    "\\sqrt{N}\\left(Q_N-I_f \\right)\\ \\xrightarrow {d} \\ N\\left(0,\\sigma ^{2}\\right)\n",
    "$$\n",
    "where $\\sigma= \\operatorname {Var}[f(\\mathbf{x})]$ does not depend on $N$\n",
    "\n",
    "So that the standard error of $Q_N$ \n",
    "$$\\sigma_{Q_N}=\\frac{{\\sigma}}{\\sqrt{N}}$$ \n",
    "decreases with the standard parametric rate, $1/\\sqrt{N}$\n",
    "\n",
    "$\\textbf{Standard errors}$: Given our estimate $Q_{N}$ of $I_g$, we can obtain an unbiased estimate of $\\sigma^2= \\operatorname {Var}[f(\\mathbf{x})]$ and the standard error of $Q_N$ \n",
    "$$\n",
    "{\\hat{\\sigma}^2_N}=\\frac{1}{N-1}\\sum _{i=1}^N (f(\\mathbf{x}_{i})-Q_{N})^2\n",
    "\\quad \\text{ and } \\quad {\\hat{\\sigma}}_{Q_N}=\\frac{{\\hat{\\sigma}}_N   }{\\sqrt{N}}$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Monte Carlo Squared: Let's study the properties of the simulated integral using Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def MC_sqaured(f,  N=500,  # Number Monte Carlo samples for each estimate\n",
    "                   S=1000, # Number Monte Carlo estimates of integral\n",
    "                   rule='random', # can be random, halton, sobol (although standard errors only are valid for random)\n",
    "               true_val=np.NaN, label='True Value', output=True): \n",
    "    Q_est=np.empty((S, 1)); Q_se=np.empty((S, 1));\n",
    "    for i_s in range(S): # do the monte carlo of our monte carlo integral \n",
    "        Q_est[i_s, 0], Q_se[i_s,0]=mc_integral(f, N=N, d=d, rule=rule)\n",
    "    if not true_val: \n",
    "        true_val=np.mean(Q_est);         \n",
    "    Q_bias=Q_est-true_val\n",
    "\n",
    "    # summary output\n",
    "    if output: \n",
    "        hist(np.array(Q_est), bins=50, theoretical=lambda x: scipy.stats.norm.pdf(x, loc=true_val, scale=np.mean(Q_se)))\n",
    "        plt.title('Monte Carlo distribution of estimates, true value of %s=%g'% (label, true_val))\n",
    "        plt.show()        \n",
    "        print('%-15s%20s%20s%20s%20s' % (label, 'E[estimate]', 'E[Bias]', 'E[Std. error]', 'Std. dev(bias)'))\n",
    "        print('%-15g%20g%20g%20g%20g' % (true_val, np.mean(Q_est), np.mean(Q_bias), np.mean(Q_se), np.std(Q_est-true_val)))\n",
    "    \n",
    "    return Q_est, Q_se, Q_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Monte Carlo Distribution: Asymptotic Normal and unbiased - Test Problem 1\n",
    "Test problem 1 (sum of d normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "N=1000;  # Number of Monte Carlo draws used to compute integral\n",
    "d=5;     # Number of dimensions in log-normal\n",
    "Î£ = build_sigma(d, Ïƒ = 1, c=0.0, print_out=False) # build variance-covariance matrix\n",
    "Î¼=1.0/d;\n",
    "f1 = lambda u: np.sum(inv_multnorm(u, Î¼, Î£=Î£) , axis=0);   \n",
    "Q1_est, Q1_se, Q1_bias = MC_sqaured(f1, N, S=5000, label='E[sum(x)]', true_val=Î¼*d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Monte Carlo Distribution: Asymptotic Normal and unbiased \n",
    "Test problem 2 (product of d log-normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "N=1000;  # Number of Monte Carlo draws used to compute integral\n",
    "d=5;     # Number of dimensions in log-normal\n",
    "Î£ = build_sigma(d, Ïƒ = 1, c=0.0, print_out=False) # build variance-covariance matrix\n",
    "Î¼=1.0/d;\n",
    "f2 = lambda u: np.exp(np.sum(inv_multnorm(u, Î¼=1.0/d, Î£=Î£) , axis=0)); \n",
    "one=np.ones((d,1)) # vector of ones used in variance formula var(sum_d(x))=a.T@Î£@a\n",
    "Q2_est, Q2_se, Q2_bias = MC_sqaured(f2, N, S=5000, label='E[prod(x)]', true_val=np.array(np.exp(Î¼*d + one.T@Î£@one/2).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rates of convergence\n",
    "- Monte Carlo using random draws converge at a rate, ${\\frac  {1}{{\\sqrt  {N}}}}$\n",
    "- Monte Carlo using pseudo random draws converge at a rate, ${\\frac{(\\log N)^{d}}{N}}$\n",
    "\n",
    "- In order for $O\\left({\\frac{(\\log N)^{d}}{N}}\\right)$ to be smaller than $O\\left({\\frac  {1}{{\\sqrt  {N}}}}\\right)$, \n",
    "the dimension $d$ needs to be small and $N$ needs to be large (e.g. $N>2^{d})$\n",
    "- Note the rates are not sharp, and these rate results is not informative of the size of the error for a given value of $N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rates of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Nvec=2**np.arange(1, 50, 1);\n",
    "fig, ax = plt.subplots(1,5,figsize=(20,4))\n",
    "dlist=[1,2,3,4,10]\n",
    "for i, d in enumerate(dlist):\n",
    "    ax[i].plot(Nvec, 1/np.sqrt(Nvec), label='Random')\n",
    "    ax[i].plot(Nvec, (np.log(Nvec)**d)/Nvec, label='Pseudo Random')\n",
    "    ax[i].set_yscale('log'); ax[i].set_xscale('log'); ax[i].set_title('d=%d'%d)\n",
    "    ax[i].set_xlabel(\"N, log scale\")\n",
    "    if i==0:\n",
    "        ax[i].set_ylabel(\"Rate of convergence, log scale\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rates of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sampling_rule='random'          # Try random, sobold and halton\n",
    "S=100                             # Number of monte carlo samples\n",
    "Nvec=np.arange(100, 3000, 100); # Number of Monte Carlo draws used to compute integral\n",
    "Î£ = build_sigma(d, Ïƒ = 1, c=0.0, print_out=False) # build variance-covariance matrix\n",
    "Î¼=1.0/d\n",
    "f1 = lambda u: np.sum(inv_multnorm(u, Î¼, Î£=Î£) , axis=0);   # Test problem 1 (product of d log-normals)\n",
    "\n",
    "Q_est=np.empty((S, len(Nvec))); Q_se=np.empty((S, len(Nvec))); Q_bias=np.empty((S, len(Nvec)))\n",
    "for i, N in enumerate(Nvec):\n",
    "    Q_est[:,[i]],Q_se[:,[i]],Q_bias[:,[i]]=\\\n",
    "    MC_sqaured(f1,N,S,sampling_rule, label='E[sum(x)]',true_val=np.array(Î¼)*d, output=False)\n",
    "\n",
    "fig, ax = plt.subplots(1,4,figsize=(30,6))\n",
    "for i in range(4): ax[i].set_xlabel(\"Number of Monte Carlo draws, N\")\n",
    "ax[0].set_title('Bias'); ax[0].plot(Nvec, np.mean(Q_bias, axis=0)); ax[0].plot(Nvec, Nvec*0, '--r'); \n",
    "ax[1].set_title('Std. err.'); ax[1].plot(Nvec, np.mean(Q_se, axis=0)); \n",
    "ax[2].set_title(r'Std. err x $\\sqrt{N}$'); ax[2].plot(Nvec,np.mean(Q_se, axis=0)*np.sqrt(Nvec), '--r'); \n",
    "ax[3].set_title(r'Std. err and $N^{-1/2}$ on log-log scale');\n",
    "ax[3].plot(Nvec,np. mean(Q_se, axis=0), '--r'); ax[3].plot(Nvec, 1/np.sqrt(Nvec))\n",
    "ax[3].set_yscale('log'); ax[3].set_xscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some lessons from this lecture\n",
    "The main advantages of Monte Carlo methods relative to quadrature methods for numerical integration are:\n",
    "1. Monte Carlo breaks curse of dimentionality\n",
    "    - Computational complexity of Quadrature Product rules increases exponentially with the dimensionality of the integral\n",
    "    - Limiting properties (including rates of convergence) for Monte Carlo methods are less unffected by dimensionality.\n",
    "    - Monte Carlo is more effective in higher dimensions\n",
    "1. Monte Carlo methods are easy to implement \n",
    "1. Monte Carlo methods are more flexible \n",
    "    - Can handle a wider range of integrands, including those with singularities or discontinuities, whereas quadrature methods may struggle in such cases.\n",
    "1. Monte Carlo methods provide error estimates\n",
    "    - Monte Carlo methods provide a natural way to estimate the error in the numerical integration\n",
    "    - Can be difficult to do with quadrature methods.\n",
    "1. Monte Carlo methods can be more accurate \n",
    "    - Monte Carlo methods can be more accurate than quadrature methods for certain types of integrands, particularly those with high variability or low regularity.\n",
    "    \n",
    "However, \n",
    "1. Quadrature is usually the best you can do in small dimensional problems\n",
    "1. Quadrature Methods can ameliorate the curse of dimensional when using sparse grids (Smolyak's algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivating example: Dynamic Portfolio Choice Model\n",
    "\n",
    "- **Bellman Equation from MDP Framework:**\n",
    "  $$\n",
    "  V_t(s_t) = \\max_{d_t \\in D(s_t)} \\left[ u_t(s_t, d_t) + \\beta \\int V_{t+1}(s_{t+1}) \\, p(ds_{t+1} \\mid s_t, d_t) \\right]\n",
    "  $$\n",
    "\n",
    "- **States:** Single state variable representing wealth, $s_t = x_t$.\n",
    "\n",
    "- **Decisions:** Multiple control variables\n",
    "  - **Consumption:** $d_t^{(1)} =c_t \\in [0, x_t]$.\n",
    "  - **Portfolio Weights:** $d_t^{(2)} =\\boldsymbol{w}_t = (w_t^{(1)}, \\ldots, w_t^{(N)})$, with $\\sum_{i=1}^N w_t^{(i)} = 1$ and $w_t^{(i)} \\geq 0$.\n",
    "\n",
    "- **Preferences:** \n",
    "Utility from consumption:\n",
    "  $$\n",
    "  u_t(s_t, d_t)=u(c_t) = \\frac{c_t^{1 - \\gamma}}{1 - \\gamma}\n",
    "  $$\n",
    "\n",
    "- **State Transition:** The distribution of next period's wealth $p(s_{t+1} \\mid s_t, d_t) = p(x_{t+1} \\mid x_t, c_t, \\boldsymbol{w}_t)$ depends on the stochastic returns of the assets:\n",
    "  $$\n",
    "  x_{t+1} = (x_t - c_t) \\left( \\sum_{i=1}^N w_t^{(i)} R_{t+1}^{(i)} \\right)\n",
    "  $$\n",
    "  where asset returns are mutivariate normal (mutually correlated):\n",
    "  $$\n",
    "  \\boldsymbol{R}_{t+1} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\n",
    "  $$\n",
    "  \n",
    "- **CHALLENGE:** Try to solve the model using MC and Quandrature (start with given portfolio weights to eleimiate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further learning resources\n",
    "\n",
    "- [https://docs.scipy.org/doc/scipy-0.14.0/reference/tutorial/integrate.html](https://docs.scipy.org/doc/scipy-0.14.0/reference/tutorial/integrate.html)  \n",
    "- [https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.statistics.html](https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.statistics.html)  \n",
    "- Docs on **SciPy.random**\n",
    "  [https://docs.scipy.org/doc/numpy-1.14.0/reference/routines.random.html](https://docs.scipy.org/doc/numpy-1.14.0/reference/routines.random.html)  \n",
    "- Docs on **SciPy.stats**\n",
    "  [https://docs.scipy.org/doc/scipy/reference/stats.html](https://docs.scipy.org/doc/scipy/reference/stats.html)  \n",
    "- Docs on **Chaospy**\n",
    "  [https://chaospy.readthedocs.io/en/master/](https://chaospy.readthedocs.io/en/master/)  \n",
    "- Random number generators [https://www.random.org/analysis](https://www.random.org/analysis)  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "date": 1612589586.079556,
  "download_nb": false,
  "filename": "01_dp_intro.rst",
  "filename_with_path": "01_dp_intro",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "title": "Dynamic Programming and Structural Econometrics #1"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
